---
title: "Unstructured Project"
author: "Jill Stafford"
format:
  html:
    toc: true
    toc-location: left
    self-contained: true
jupyter: python3
---
Project description: Looking at the differences between global Catholic populations from 1910 to 2010, I want to understand what may have caused or contributed to these changes in the rising and falling of numbers.

Index:
-Scraping Images
-Optical Character Recognition (OCR)
-Population Growth Factor Calculations
-Topic Modeling (3 types)
-Sentiment Analysis (2 types)
-Regex Content Extraction
-Conclusion

## Image Scraping
Here I scraped Catholic Population images from pew research that contained statistics from 1910 and 2010(it was a very tough process of trial and error.) 
```{python}
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import os

chart_pages = [
    "https://www.pewresearch.org/religion/2013/02/13/the-global-catholic-population/pf_13-02-13_global-catholics_chart-3/",
    "https://www.pewresearch.org/religion/2013/02/13/the-global-catholic-population/pf_13-02-13_global-catholics_chart-4/"
]

headers = {"User-Agent": "Mozilla/5.0"}
os.makedirs("charts", exist_ok=True)

chart_imgs = []

for page in chart_pages:
    r = requests.get(page, headers=headers)
    soup = BeautifulSoup(r.text, "html.parser")

    img = soup.find("img")
    if not img:
        print("[!] No image found on:", page)
        continue

    src = img.get("src")
    if src:
        full = urljoin(page, src)
        chart_imgs.append(full)

print("[+] Found chart images:")
for c in chart_imgs:
    print("   ", c)

for i, img_url in enumerate(chart_imgs, start=1):

    clean_url = img_url.split("?")[0]

    img_data = requests.get(clean_url, headers=headers).content
    ext = os.path.splitext(clean_url)[1]
    filename = f"charts/chart_{i}{ext}"

    with open(filename, "wb") as f:
        f.write(img_data)

    print("[âœ“] Saved:", filename)

```

Here I displayed the images I extracted to make sure that the scraping worked.
```{python}
from IPython.display import Image, display

display(Image(filename="charts/chart_1.png"))
display(Image(filename="charts/chart_2.png"))

```

## Optical Character Recognition (OCR)
I downloaded Tesseract to do OCR and scrape the statistical information from the images
```{python}
import pytesseract
from PIL import Image
import re
import pandas as pd

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"
print(pytesseract.get_tesseract_version())
```

Here I printed the scraped data from one of the images to confirm it worked before the next step.
```{python}
img = Image.open("charts/chart_2.png") 
text = pytesseract.image_to_string(img, config="--psm 6")
print(text)
```

Here I ran OCR again, created an empty table, looped through the extracted text of data lines, checked if a line contained a population number, extracted the info from the line, appended the extracted row, and then returned the dataframe.
```{python}
def extract_table_from_chart(path, year):
    img = Image.open(path)

    raw = pytesseract.image_to_string(img, config="--psm 6")

    rows = []

    for line in raw.splitlines():
        line = line.strip()
        if not line:
            continue

        skip_fragments = [
            "10 Countries", "Countries", "ESTIMATED", "PERCENTAGE",
            "World Total", "Population estimates", "Figures for 1910",
            "Pew Research"
        ]
        if any(s.lower() in line.lower() for s in skip_fragments):
            continue

        if not re.search(r"\d[\d,]*,\d{3},\d{3}", line) and not re.search(r"\.\d[\d,]*,\d{3},\d{3}", line):
            continue

        m_country = re.match(r"^[A-Za-z .]+", line)
        if not m_country:
            continue
        country = m_country.group(0).strip().title()

        m_pop = re.search(r"\.?\d[\d,]*,\d{3},\d{3}", line)
        if not m_pop:
            continue
        pop_str = m_pop.group(0).replace(".", "") 
        pop = int(pop_str.replace(",", ""))

        rest = line[m_pop.end():]

        nums = re.findall(r"[\d\.]+", rest)

        pct_cat = None
        pct_world = None
        if len(nums) >= 2:
            pct_cat = float(nums[-2])
            pct_world = float(nums[-1])

        rows.append({
            "Country": country,
            "Year": year,
            "Catholic_Pop": pop,
            "Percent_Catholic": pct_cat,
            "Percent_World_Catholics": pct_world
        })

    return pd.DataFrame(rows)
```

Here I printed the charts, but they got mixed up.
```{python}
df_1910 = extract_table_from_chart("charts/chart_1.png", 1910)
df_2010 = extract_table_from_chart("charts/chart_2.png", 2010)

print("1910:")
display(df_1910)

print("2010:")
display(df_2010)

```

Here I corrected the information and reinforced the 1910 dataframe.
```{python}
import pandas as pd

data_1910 = {
    "Country": [
        "France", "Italy", "Brazil", "Spain", "Poland",
        "Germany", "Mexico", "United States",
        "Philippines", "Czech Republic"
    ],
    "Catholic_Pop_1910": [
        40510000, 35270000, 21430000, 20350000, 18750000,
        16580000, 14280000, 12470000, 7260000, 7120000
    ],
    "Percent_Catholic_1910": [
        98.4, 99.9, 95.6, 99.9, 77.1,
        35.7, 91.0, 14.2, 78.7, 86.2
    ],
    "Percent_World_1910": [
        13.9, 12.1, 7.4, 7.0, 6.4,
        5.7, 4.9, 4.3, 2.5, 2.4
    ]
}

df1910 = pd.DataFrame(data_1910)
df1910

```

I printed the extracted information.
```{python}
df_1910 = extract_table_from_chart("charts/chart_1.png", 1910)
df_2010 = extract_table_from_chart("charts/chart_2.png", 2010)
print(df_1910)
print(df_2010)
```

Here I swapped the dataframes to make sure the data had the correct years.
```{python}
df_1910, df_2010 = df_2010.copy(), df_1910.copy()
df_1910["Year"] = 1910
df_2010["Year"] = 2010
```

Next I fixed the typos in the names of the countries
```{python}
df_1910["Country"] = df_1910["Country"].replace({
    "Aly": "Italy",
    "Democratic Republic Of The Congo": "Democratic Republic of the Congo",
    "Italy .": "Italy"
})

df_2010["Country"] = df_2010["Country"].replace({
    "Aly": "Italy",
    "Italy .": "Italy"
})

```

Lastly I made a "truth dictionary" for each chart to make sure the data can be easily analyzed.
```{python}
truth_1910 = {
    "France": (40510000, 98.4, 13.9),
    "Italy": (35270000, 99.9, 12.1),
    "Brazil": (21430000, 95.6, 7.4),
    "Spain": (20350000, 99.9, 7.0),
    "Poland": (18750000, 77.1, 6.4),
    "Germany": (16580000, 35.7, 5.7),
    "Mexico": (14280000, 91.0, 4.9),
    "United States": (12470000, 14.2, 4.3),
    "Philippines": (7260000, 78.7, 2.5),
    "Czech Republic": (7120000, 86.2, 2.4),
}

```

```{python}
truth_2010 = {
    "Brazil": (126750000, 65.0, 11.7),
    "Mexico": (96450000, 85.0, 8.9),
    "Philippines": (75570000, 81.0, 7.0),
    "United States": (75380000, 24.3, 7.0),
    "Italy": (49170000, 81.2, 4.6),
    "Colombia": (38100000, 82.3, 3.5),
    "France": (37930000, 60.4, 3.5),
    "Poland": (35310000, 92.2, 3.3),
    "Spain": (34670000, 75.2, 3.2),
    "Democratic Republic of the Congo": (31210000, 47.3, 2.9),
}
```

Here I used the apply_truth function to overwrite what had been scraped using OCR. Then I looped it through the rows in the data frame.
```{python}
def apply_truth(df, truth_dict):
    for idx, row in df.iterrows():
        country = row["Country"]
        if country in truth_dict:
            pop, pct_cat, pct_world = truth_dict[country]
            df.loc[idx, ["Catholic_Pop",
                         "Percent_Catholic",
                         "Percent_World_Catholics"]] = [pop, pct_cat, pct_world]
    return df

df_1910 = apply_truth(df_1910, truth_1910)
df_2010 = apply_truth(df_2010, truth_2010)

```

Here I printed out the clean dataframes
```{python}
print("1910 clean:")
display(df_1910)

print("2010 clean:")
display(df_2010)
```

## Now to compute population change growth numbers 
Here I merged the dataframe to easily compute the growth factor and percent change. 
```{python}
import pandas as pd

df_1910_small = df_1910[["Country", "Catholic_Pop"]].copy()
df_2010_small = df_2010[["Country", "Catholic_Pop"]].copy()

df_1910_small = df_1910_small.rename(columns={"Catholic_Pop": "Catholic_Pop_1910"})
df_2010_small = df_2010_small.rename(columns={"Catholic_Pop": "Catholic_Pop_2010"})

df_merge = df_1910_small.merge(df_2010_small, on="Country", how="inner")

df_merge["GrowthFactor"] = df_merge["Catholic_Pop_2010"] / df_merge["Catholic_Pop_1910"]
df_merge["Percent_Change"] = (
    (df_merge["Catholic_Pop_2010"] - df_merge["Catholic_Pop_1910"])
    / df_merge["Catholic_Pop_1910"]
)
df_merge
```

Then i sorted by the greatest growth to see which country had the most dramatic changes.
```{python}
df_merge = df_merge.sort_values("GrowthFactor", ascending=False).reset_index(drop=True)

df_merge
```

Lastly I made a bar chart visualizing the growth changes.
```{python}
import matplotlib.pyplot as plt

plt.figure()
plt.bar(df_merge["Country"], df_merge["GrowthFactor"])
plt.xticks(rotation=45, ha="right")
plt.ylabel("Growth Factor (2010 / 1910)")
plt.title("Catholic Population Growth Factor by Country")
plt.tight_layout()
```

## Now to pull historic info from each country
Here I used beautiful soup again to pull information from different wikipedia pages about the Catholic Church in those countries. I was hoping that it would provide unbiased information compared to a news source that might be biased politically or religiously. Additionally, they have a verification system that removes any misinformation pretty soon after posting, so this helped my reservations in using Wikipedia for the project.
```{python}
import requests
from bs4 import BeautifulSoup

countries = df_merge["Country"].tolist()

wiki_slugs = {
    "Philippines": "Catholic_Church_in_the_Philippines",
    "Mexico": "Catholic_Church_in_Mexico",
    "United States": "Catholic_Church_in_the_United_States",
    "Brazil": "Catholic_Church_in_Brazil",
    "Poland": "Catholic_Church_in_Poland",
    "Spain": "Catholic_Church_in_Spain",
    "Italy": "Catholic_Church_in_Italy",
    "France": "Catholic_Church_in_France",
}

```

Here I created a user-agent to access teh wikipedia pages to get past potential blockages. Then I downloaded the webpages and parsed through them to get the main text blocks. This block essentially defines the scraping logic and how to go about it.
```{python}
import requests
from bs4 import BeautifulSoup

headers = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    )
}

def scrape_country_text(country):
    slug = wiki_slugs[country]
    url = f"https://en.wikipedia.org/wiki/{slug}"

    resp = requests.get(url, headers=headers)
    resp.raise_for_status()  

    soup = BeautifulSoup(resp.text, "html.parser")

    content_div = soup.select_one("div.mw-parser-output")
    if content_div is None:
        return ""

    paragraphs = []

    for p in content_div.find_all("p", recursive=False):
        text = p.get_text(" ", strip=True)
        if text:
            paragraphs.append(text)

    if not paragraphs:
        for p in content_div.find_all("p"):
            text = p.get_text(" ", strip=True)
            if text:
                paragraphs.append(text)

    return " ".join(paragraphs)

```

Then I perform the actual scraping.
```{python}
rows = []

for country in countries:
    print(f"Scraping {country}...")
    try:
        text = scrape_country_text(country)
    except Exception as e:
        print(f"  Error scraping {country}: {e}")
        text = ""  

    rows.append({"Country": country, "Text": text})

wiki_df = pd.DataFrame(rows)
print("Number of rows:", len(wiki_df))
display(wiki_df)
```

Now I'll clean the text. I imported English_Stop_Words to get rid of common words that might accidentally get picked up. I also changed everything to all lowercase so it limits variation. Then I got rid of puncuation, numbers and special characters. Then I tokenized it to get it ready for topic modeling.
```{python}
import re
from sklearn.feature_extraction.text 
import ENGLISH_STOP_WORDS

def clean_text(t):
    if not isinstance(t, str):
        return ""
    t = t.lower()
    t = re.sub(r"[^a-z\s]", " ", t)
    tokens = t.split()
    tokens = [w for w in tokens if len(w) > 2 and w not in ENGLISH_STOP_WORDS]
    return " ".join(tokens)

wiki_df["Clean_Text"] = wiki_df["Text"].apply(clean_text)
wiki_df[["Country", "Clean_Text"]]
```

## Topic Modeling 1
Here I imported the necessary packages for vectorization. Then I split the countries into 2 different growth groups. Then I vectorized the text with various parameters. I ignored words that appeared in 95% of documents and words that appeared in only one document. This way it takes out super general words and allows for general themes to come through.
```{python}
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

high_growth = ["Philippines", "Mexico", "United States", "Brazil"]
low_growth  = ["France", "Italy", "Spain", "Poland"]

vectorizer = CountVectorizer(max_df=0.95, min_df=2)  
X = vectorizer.fit_transform(wiki_df["Clean_Text"])

feature_names = vectorizer.get_feature_names_out()
```

Here I defined the amount of topics I wanted. Then i set up the LDA model. I told it to build 3 topics, and to have a random state of 42, making it partly random and reproducible. I also told it to use everything at once using batch.
```{python}
n_topics = 3

lda = LatentDirichletAllocation(
    n_components=n_topics,
    random_state=42,
    learning_method="batch"
)
lda.fit(X)

```

Here I printed out the top words for each topic.
```{python}
def print_top_words(model, feature_names, n_top_words=10):
    for topic_idx, topic in enumerate(model.components_):
        top_indices = topic.argsort()[:-n_top_words - 1:-1]
        top_words = [feature_names[i] for i in top_indices]
        print(f"Topic #{topic_idx}: {' | '.join(top_words)}")

print_top_words(lda, feature_names, n_top_words=10)
```

# Topic Distribution
Here I showed the way that each topic was distributed among the countries. I was hoping to see some patterns among high growth and low growth. It appears that low growth countries really liked topic 1 (2), which the high growth countries really liked topics 0 and 2 (1&3)
```{python}
doc_topic_dist = lda.transform(X)  

for i in range(n_topics):
    wiki_df[f"Topic_{i}_prob"] = doc_topic_dist[:, i]

wiki_df[["Country"] + [f"Topic_{i}_prob" for i in range(n_topics)]]
```

Here I continued with that analysis and calculated average topic probabilities. This reinforced the same topic finding associations from above.
```{python}
wiki_df["Growth_Group"] = wiki_df["Country"].apply(
    lambda c: "High" if c in high_growth else "Low"
)

topic_cols = [f"Topic_{i}_prob" for i in range(n_topics)]

group_topics = wiki_df.groupby("Growth_Group")[topic_cols].mean()
group_topics

```

## Topic Modeling 2
I wanted to try and continue to find patterns with topic modeling. So I did high growth key words versus low growth key words.
```{python}
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd
import numpy as np

wiki_df["Growth_Group"] = wiki_df["Country"].apply(
    lambda c: "High" if c in high_growth else "Low"
)

group_texts = wiki_df.groupby("Growth_Group")["Clean_Text"].apply(lambda x: " ".join(x))

vectorizer = TfidfVectorizer(max_features=2000)
tfidf_matrix = vectorizer.fit_transform(group_texts)

feature_names = np.array(vectorizer.get_feature_names_out())

tfidf_df = pd.DataFrame(
    tfidf_matrix.toarray(),
    index=group_texts.index,
    columns=feature_names
)

top_high = tfidf_df.loc["High"].sort_values(ascending=False).head(20)
top_low = tfidf_df.loc["Low"].sort_values(ascending=False).head(20)

print("Top High-Growth Keywords:\n", top_high)
print("\nTop Low-Growth Keywords:\n", top_low)
```

High growth topics versus Low Growth Topics. It didn't give much more insight.
```{python}
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

def lda_for_group(group_name):
    texts = wiki_df[wiki_df["Growth_Group"] == group_name]["Clean_Text"].tolist()
    vectorizer = CountVectorizer(max_df=0.95, min_df=1)
    X = vectorizer.fit_transform(texts)
    lda = LatentDirichletAllocation(n_components=2, random_state=42)
    lda.fit(X)
    return lda, vectorizer.get_feature_names_out()

def print_topics(model, feature_names, n_top=12):
    for idx, topic in enumerate(model.components_):
        top_indices = topic.argsort()[:-n_top - 1:-1]
        words = [feature_names[i] for i in top_indices]
        print(f"Topic #{idx}: {' | '.join(words)}\n")

lda_high, feat_high = lda_for_group("High")
print("\n=== HIGH-GROWTH TOPICS ===")
print_topics(lda_high, feat_high)

lda_low, feat_low = lda_for_group("Low")
print("\n=== LOW-GROWTH TOPICS ===")
print_topics(lda_low, feat_low)
```

## Topic modeling (attempt 3:(stricter))
I had felt like The words from the first rounds of topic modeling above weren't very helpful. So I redid it with many stopwords that somewhat helped to bring forth more unique and characteristic words. I was really just hoping to bring about stronger insights. I also narrowed it down to just 2 topics in hopes of having a clearer distinction between the themes that emerge. I realized I accidentally deleted a crucial code block with the "clean text" column so I added it back in here, before going through with the new topic modeling.

```{python}
import re
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS

def clean_text(t):
    if not isinstance(t, str):
        return ""
    t = t.lower()
    t = re.sub(r"[^a-z\s]", " ", t)
    tokens = t.split()
    tokens = [w for w in tokens if len(w) > 2 and w not in ENGLISH_STOP_WORDS]
    return " ".join(tokens)

wiki_df["Clean_Text"] = wiki_df["Text"].apply(clean_text)
```

new topic modeling method
```{python}
from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS
from sklearn.decomposition import LatentDirichletAllocation


extra_stopwords = {
    "population", "brazil", "mexico", "spanish", "philippines", "religious", "approximately",
    "united", "states", "catholics", "largest", "country", "church", "catholic",
    "census", "million", "survey", "national",
    "rome", "cathedral", "dioceses", "bishop",
    "century", "peter", "city", "conference",
    "world", "state", "period", "identified",
    "christian", "catholicism", "members", "number", "data", "worldwide", "countries",
    "according", "bishops", "statistics", "spiritual", "leadership", "holy", "previous", "estimated", "having", "communion", "latin", "second", "episcopal", "society", "includes", "year", "beginning", "early", "established", "served",
    "nations", "primate", "eparchies", "surveyed", "adherents"}

custom_stop = list(ENGLISH_STOP_WORDS.union(extra_stopwords))

vectorizer = CountVectorizer(
    max_df=0.95,
    min_df=2,
    stop_words=custom_stop,
    ngram_range=(1, 2) 
)

X = vectorizer.fit_transform(wiki_df["Clean_Text"])
feature_names = vectorizer.get_feature_names_out()

n_topics = 2

lda = LatentDirichletAllocation(
    n_components=n_topics,
    random_state=42,
    learning_method="batch"
)
lda.fit(X)

def print_top_words(model, feature_names, n_top_words=10):
    for topic_idx, topic in enumerate(model.components_):
        top_indices = topic.argsort()[:-n_top_words - 1:-1]
        top_words = [feature_names[i] for i in top_indices]
        print(f"Topic #{topic_idx}: {' | '.join(top_words)}")

print_top_words(lda, feature_names, n_top_words=10)

```

# Topic Distribution
Here I ran the topic distribution again to see how it may have been affected by the stop words.
```{python}
doc_topic_dist = lda.transform(X)  

for i in range(n_topics):
    wiki_df[f"Topic_{i}_prob"] = doc_topic_dist[:, i]

wiki_df[["Country"] + [f"Topic_{i}_prob" for i in range(n_topics)]]

```

Here I split it into high growth and low growth again. I found that topic 0 is more heavily leaned into by the high growth group. While the low growth group more heavily leans into Topic 1. It seems as if topic 0 has themes of evangelization, while topic 1 has themes of historical influence. Which would correlate with findings of growth vs decay.
```{python}
wiki_df["Growth_Group"] = wiki_df["Country"].apply(
    lambda c: "High" if c in high_growth else "Low"
)

topic_cols = [f"Topic_{i}_prob" for i in range(n_topics)]

group_topics = wiki_df.groupby("Growth_Group")[topic_cols].mean()
group_topics
```

## Sentiment Analysis Score per country (1)
Next I was curious to do a sentiment analysis, because why not? Actually I was curious if certain words in alignment with growth versus decay might show up here. I created positive and negative sets of words.
```{python}
positive_words = {
    "growth", "expansion", "revival", "mission", "missions",
    "evangelization", "conversion", "conversions", "renewal",
    "recognition", "rights", "freedom", "liberty"
}

negative_words = {
    "decline", "secularization", "persecution", "persecutions",
    "repression", "scandal", "scandals", "crisis", "crises",
    "conflict", "war", "wars", "suppression", "anti", "hostility"
}

```

Then I did the actual analysis. I split the text into individual token words, separately counted how many were in the positive and negative lists, and then created a score between -1 and 1. Unfortunately, only negative words got picked up on. So there wasn't much insight here.
```{python}
def simple_sentiment_score(text):
    if not isinstance(text, str):
        return 0.0
    tokens = text.split()
    if not tokens:
        return 0.0

    pos = sum(1 for t in tokens if t in positive_words)
    neg = sum(1 for t in tokens if t in negative_words)

    return (pos - neg) / len(tokens)

wiki_df["Sentiment_Score"] = wiki_df["Clean_Text"].apply(simple_sentiment_score)

wiki_df[["Country", "Sentiment_Score"]]

```

I split the sentiment score by growth group, but it didn't really give anymore information.
```{python}
sent_group = wiki_df.groupby("Growth_Group")["Sentiment_Score"].mean()
sent_group

```

Next I made a bar chart showing the sentiment analysis.
```{python}
plt.figure()
plt.bar(wiki_df["Country"], wiki_df["Sentiment_Score"])
plt.xticks(rotation=45, ha="right")
plt.ylabel("Sentiment Score (simple lexicon)")
plt.title("Sentiment of Catholic History Text by Country")
plt.tight_layout()
```

## Attempting supervised machine learning sentiment analysis (2)
I wanted to try a different form of sentiment analysis without my own chosen words. Here I used regex to break the wikipedia pages down by sentences.
```{python}
def split_into_sentences(text):
    import re
    if not isinstance(text, str):
        return []
    text = re.sub(r'([.!?])', r'\1 ', text)
    sentences = re.split(r'(?<=[.!?])\s+', text)
    return [s.strip() for s in sentences if s.strip()]
```

Next i went through each countries Wikipedia pages and collected the sentences into a list. I also made sure to only get sentences longer than 6 words in order to avoid fragments and titles.
```{python}
all_sentences = []

for text in wiki_df["Text"]:
    for s in split_into_sentences(text):
        if len(s.split()) > 6:  
            all_sentences.append(s)

```

Here I imported a set of randomly shuffled sentences that I will later manually label.
```{python}
import random

random.shuffle(all_sentences)
sample_sentences = all_sentences[:120]   
```

Here I created a data frame that will allow me to store their sentiment labels.
```{python}
import pandas as pd

df_label = pd.DataFrame({
    "sentence": sample_sentences,
    "label": [""] * len(sample_sentences)  
})

df_label.head(10)
```

Next I exported the the table into a CSV file.
```{python}
df_label.to_csv("labeling_sentences.csv", index=False)
```

Here I manually labeled 83 rows of data
(And saved as a different file name so that if the above block is rerun, the labels wont be erased). Unforunately learned that one the hard way when running through my code and accidentally erased all my labels :_(
```{python}
df_label = pd.read_csv("labeling_sentences1.csv")
df_label
```

Here I dropped rows with neutral labels. Then I counted how many rows are positive or negative.
```{python}
df_label = df_label[df_label["label"] != "neutral"].reset_index(drop=True)
df_label["label"].value_counts()
```

Here I imported the Machine Learning packages I would need to create the pipeline.
```{python}
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score
import pandas as pd
```

Here I defined my training data(75%) and testing data(25%). I also stratified it to make sure that there was a fair representation of labels among the data. 
```{python}
X_train, X_test, y_train, y_test = train_test_split(
    df_label["sentence"],
    df_label["label"],
    test_size=0.25,
    random_state=42,
    stratify=df_label["label"]
)
```

Next I vectorized the data by converting the sentences to numeric features that captured the importance of the words. I integrated stop words and limited the vocabulary to 5000.
```{python}
tfidf = TfidfVectorizer(
    stop_words="english",
    max_features=5000,
    ngram_range=(1,2)   
)

X_train_vec = tfidf.fit_transform(X_train)
X_test_vec = tfidf.transform(X_test)
```

Here I ran the logistic regression. I kept the max iteration to 300 to give it enough to converge but not too much to make it take forever.
```{python}
clf = LogisticRegression(max_iter=300)
clf.fit(X_train_vec, y_train)
```

Next I evaluated the performance of the model. I got an acuracy of .67, but also a precision score of 0.00 for negative sentiments becasue it labeled all negative sentences incorrectly. This is probably due to the training set being so small.
```{python}
y_pred = clf.predict(X_test_vec)

print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))
```

This is the last step where I used the model on each country's wikipedia page. I turned each whole text into a TF-IDF vector using the same vocabulary. Then I gave probabilities to each class of pos or neg. Next I extracted those probabilities to create the sentiment analysis score and finally gave a hard label to the sentiment score. The logistic regression model results point to all pages having a positive sentiment score. The highest growth country did have the highest sentiment score which was a positive. However there was no clear distinction of sentiment among high growth and low growth groups. 
```{python}
wiki_vec = tfidf.transform(wiki_df["Text"])
probs = clf.predict_proba(wiki_vec)

wiki_df["ML_Sentiment_Score"] = probs[:, list(clf.classes_).index("positive")]
wiki_df["ML_Sentiment_Label"] = clf.predict(wiki_vec)

wiki_df[["Country", "ML_Sentiment_Score", "ML_Sentiment_Label"]]
```

## Regex content extraction
I wanted to keep trying to see if I could find more details that would help me solve the question. This time I used regex mthods to get the dates during the time period of 1910-2010.

```{python}
import re

def extract_events_1910_2010(text):
    if not isinstance(text, str):
        return []

    year_regex = r"(19[1-9]\d|200\d|2010)"
    pattern = year_regex + r".{0,80}"

    matches = re.findall(pattern, text)

    return matches
```

Only 4 countries had dates show up.
```{python}
wiki_df["Events_1910_2010"] = wiki_df["Text"].apply(extract_events_1910_2010)
wiki_df[["Country", "Events_1910_2010"]]

```

Then I picked out the sentences surrounding those years to see what further details might appear. The results here aren't super insightful. The only one that really sticks out to me is how in the year 2000 Poland had baptized 99% of all children born.
```{python}
def extract_events_1910_2010_structured(text):
    if not isinstance(text, str):
        return []
    
    pattern = r"(19[1-9]\d|200\d|2010)(.{0,80})"
    results = []

    for match in re.findall(pattern, text):
        year = match[0]
        context = match[1].strip()
        results.append((int(year), context))

    return results

wiki_df["Events_1910_2010_structured"] = wiki_df["Text"].apply(extract_events_1910_2010_structured)
wiki_df[["Country", "Events_1910_2010_structured"]]

```


## Conclusion
My original question was trying to understand why different country's Catholic populations grew exponentially while others diminished. I went through a long series of Unstructured Analytic methods in order to find the answer. 

First I started by scraping the Pew research center for 2 images that detailed statistics from 1910 to 2010. From there I used Optical Character Recognition to try and scrape the information from the images. I would have just scraped the words from the article itself, however the data was only contained in the images. Which led to a fun unstructured problem of downloading and using the Tesseract, which made me feel like I was in a Marvel Avengers movie. OCR made a few mistakes which I corrected and then I computed the growth factor and percent change. The Phillipines had the greatest then following in order was Mexico, the US, Brazil, Poland, Spain, Italy, and France. It was quite interesting because France had the largest Catholic population in 1910 but then "fell from the graces" of country population growth lol. 

From there I scraped the Wikipedia pages for each country concerning the presence of the Catholic Church. I did multiple types of Topic modeling, sentiment analyses, and regex content extraction. My hope was that there would be a glaringly clear conclusion. There never really was which was why I was so motivated to try out so many variations of the unstructured analytics tools that we learned in class. The most insightful tool was topic modeling attempt #3. High growth countries fell into topic 0, low growth countries fell into topic 1. Topic 0 had themes of evangelization, while topic 1 had themes of historical influence. Which would correlate with findings of growth vs decay. 

I had been hoping that the sentiment analysis might find overwhelmingly negative and positive words with a greater divide. Unfortunately Wikipedia pages contain pretty neutral text so most of the analysis was pretty balancced among high and low growth. The last method of regex content extraction was used in hopes that there might be a stark historical event that may have had a large impact on these population numbers. Unfortunately that wasn't much of the case either. 

All in all, this project was a test of patience and perserverence with little to show for it. I wish I had better results, but it was a fun process exploring ideas from class in the context of global Catholic history.